{"cells":[{"cell_type":"markdown","source":["# [Scalable Data Science](http://www.math.canterbury.ac.nz/~r.sainudiin/courses/ScalableDataScience/)\n\n\n### prepared by [Akinwande Atanda](https://nz.linkedin.com/in/akinwande-atanda)\n\n*supported by* [![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/databricks_logoTM_200px.png)](https://databricks.com/)\nand \n[![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/AWS_logoTM_200px.png)](https://www.awseducate.com/microsite/CommunitiesEngageHome)\n\n\n## [Tweet Analytics](https://github.com/aaa121/Spark-Tweet-Streaming-Presentation-May-2016)"],"metadata":{}},{"cell_type":"markdown","source":["## Creating Machine Learning Pipeline without Loop\n* The elasticNetParam coefficient is fixed at 1.0\n* Read the Spark ML documentation for Logistic Regression\n* The dataset \"pos_neg_category\" can be split into two or three categories as done in the next note. In this note, the dataset is randomly split into training and testing data\n* This notebook can be upload to create a job for scheduled training and testing of the logistic classifier algorithm"],"metadata":{}},{"cell_type":"markdown","source":["#### Import the required python libraries:\n* From PySpark Machine Learning module import the following packages:\n  * Pipeline; \n  * binarizer, tokenizer and hash tags from feature package; \n  * logistic regression from regression package;\n  * Multi class evaluator from evaluation package\n* Read the [PySpark ML package](http://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression) documentation for more details"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import *\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import *\nfrom pyspark.ml.classification import *\nfrom pyspark.ml.tuning import *\nfrom pyspark.ml.evaluation import *\nfrom pyspark.ml.regression import *"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["#### Set the Stages (Binarizer, Tokenizer, Hash Text Features, and Logistic Regression Classifier Model)"],"metadata":{}},{"cell_type":"code","source":["bin = Binarizer(inputCol = \"category\", outputCol = \"label\", threshold = 0.5) # Positive reviews > 0.5 threshold\ntok = Tokenizer(inputCol = \"review\", outputCol = \"word\") #Note: The column \"words\" in the original table can also contain sentences that can be tokenized\nhashTF = HashingTF(inputCol = tok.getOutputCol(), numFeatures = 50000, outputCol = \"features\")\nlr = LogisticRegression(maxIter = 10, regParam = 0.0001, elasticNetParam = 1.0)\npipeline = Pipeline(stages = [bin, tok, hashTF, lr])"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["#### Convert the imported featurized dataset to dataframe"],"metadata":{}},{"cell_type":"code","source":["df = table(\"pos_neg_category\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["#### Randomly split the dataframe into training and testing set"],"metadata":{}},{"cell_type":"code","source":["(trainingData, testData) = df.randomSplit([0.7, 0.3])"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### Fit the training dataset into the pipeline"],"metadata":{}},{"cell_type":"code","source":["model = pipeline.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["#### Test the predictability of the fitted algorithm with test dataset"],"metadata":{}},{"cell_type":"code","source":["predictionModel=model.transform(testData)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["display(predictionModel.select(\"label\",\"prediction\", \"review\", \"probability\")) # Prob of being 0 (negative) against 1 (positive)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["predictionModel.select(\"label\",\"prediction\", \"review\", \"probability\").show(10) # Prob of being 0 (negative) against 1 (positive)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["#### Access the accuracy of the algorithm"],"metadata":{}},{"cell_type":"code","source":["evaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"precision\")\naccuracy = evaluator.evaluate(predictionModel)\n\nprint(\"Logistic Regression Classifier Accuracy Rate = %g \" % (accuracy))\nprint(\"Test Error = %g \" % (1.0 - accuracy))\n"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["# [Scalable Data Science](http://www.math.canterbury.ac.nz/~r.sainudiin/courses/ScalableDataScience/)\n\n\n### prepared by [Akinwande Atanda](https://nz.linkedin.com/in/akinwande-atanda)\n\n*supported by* [![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/databricks_logoTM_200px.png)](https://databricks.com/)\nand \n[![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/AWS_logoTM_200px.png)](https://www.awseducate.com/microsite/CommunitiesEngageHome)\n\n\n## [Tweet Analytics](https://github.com/aaa121/Spark-Tweet-Streaming-Presentation-May-2016)"],"metadata":{}}],"metadata":{"name":"TA03_02_binary_classification","notebookId":128354},"nbformat":4,"nbformat_minor":0}
